# Implementation Plan for Preference-Guided Group Relative Policy Optimization (PG-GRPO)

### 1. Math-to-Code Breakdown

Here is a mapping of the mathematical concepts in GRPO and DPO to their respective code implementations.

#### Group Relative Policy Optimization (GRPO)

| Mathematical Concept        | Formula                                                                                                                                                                            | Code Implementation                                                                                                                                                                                                                                                  | Explanation                                                                                                                                                                                                                                                                                                                 |
| --------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Probability Ratio**       | $r_i(\theta) = \frac{\pi_\theta(r_i\|p)}{\pi_{\theta_{\text{old}}}(r_i\|p)}$                                                                                                       | `log_ratio = per_token_logps - old_per_token_logps` <br> `coef_1 = torch.exp(log_importance_weights)`                                                                                                                                                                | The code first calculates the log of the probability ratio for efficiency and numerical stability. It then exponentiates this to get the actual ratio, `coef_1`.                                                                                                                                                            |
| **Advantage Estimate**      | $A_i$                                                                                                                                                                              | `advantages = inputs["advantages"]`                                                                                                                                                                                                                                  | The advantage `A_i` is pre-computed and passed into the loss function via the `inputs` dictionary. This value represents the reward of a response minus the average reward of its group.                                                                                                                                    |
| **Clipped Surrogate Loss**  | $\mathcal{L}_{\text{clip}}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \left( \min \left( r_i(\theta) A_i, \text{clip} ( r_i(\theta), 1-\varepsilon, 1+\varepsilon ) A_i \right) \right)$ | `coef_2 = torch.clamp(coef_1, 1 - self.epsilon_low, 1 + self.epsilon_high)` <br> `per_token_loss1 = coef_1 * advantages.unsqueeze(1)` <br> `per_token_loss2 = coef_2 * advantages.unsqueeze(1)` <br> `per_token_loss = -torch.min(per_token_loss1, per_token_loss2)` | The code calculates the two terms inside the `min` function separately (`per_token_loss1` and `per_token_loss2`). `coef_2` is the clipped version of the probability ratio. The `torch.min` function then selects the smaller of the two, and the negative sign is applied because PyTorch optimizers perform minimization. |
| **KL Divergence Penalty**   | $\mathbb{D}_{\text{KL}}(\pi_\theta \| \pi_{\text{orig}})$                                                                                                                          | `ref_per_token_logps = inputs["ref_per_token_logps"]` <br> `per_token_kl = (torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1)`                                                                                         | This is a common way to approximate KL divergence. It's added to the `per_token_loss` if `self.beta` is non-zero.                                                                                                                                                                                                           |
| **GRPO Objective Function** | $\mathcal{L}_{\text{GRPO}}(\theta) = \mathcal{L}_{\text{clip}}(\theta) - w_1 \mathbb{D}_{\text{KL}}(\pi_\theta \| \pi_{\text{orig}})$                                              | `if self.beta != 0.0: per_token_loss = per_token_loss + self.beta * per_token_kl` <br> `loss = ((per_token_loss * completion_mask).sum(-1) / completion_mask.sum(-1).clamp(min=1.0)).mean()`                                                                         | The final loss is a combination of the clipped surrogate loss and the KL penalty (weighted by `self.beta`). The result is then masked to only include the completion tokens and averaged over the batch.                                                                                                                    |

#### Direct Preference Optimization (DPO)

| Mathematical Concept       | Formula                                                                                                                                                                                                                                  | Code Implementation                                                                                                                                                                                                                            | Explanation                                                                                                                                                                                                                                                                |
| -------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Log-Probability Ratios** | $\log \frac{\pi_\theta(y_w                                                                                \| x)}{\pi_{\text{ref}}(y_w \| x)}$ and $\log \frac{\pi_\theta(y_l \| x)}{\pi_{\text{ref}}(y_l \| x)}$                         | `_losses, _chosen_rewards, _rejected_rewards = self.dpo_loss(model_output["chosen_logps"], model_output["rejected_logps"], ref_chosen_logps, ref_rejected_logps, loss_type, model_output)`                                                     | The `dpo_loss` function takes the log probabilities of the chosen and rejected responses from both the current policy (`model_output`) and the reference policy (`ref_..._logps`) as input to compute these ratios internally.                                             |
| **DPO Loss Function**      | $\mathcal{L}_{\text{DPO}} = - \mathbb{E} \left[ \log \sigma \left( \beta \left( \log \frac{\pi_\theta(y_w \| x)}{\pi_{\text{ref}}(y_w \| x)} - \log \frac{\pi_\theta(y_l     \| x)}{\pi_{\text{ref}}(y_l \| x)} \right) \right) \right]$ | `losses = 0` <br> `for idx, loss_type in enumerate(self.loss_type):` <br> &nbsp;&nbsp;&nbsp;&nbsp; `_losses, ... = self.dpo_loss(...)` <br> &nbsp;&nbsp;&nbsp;&nbsp; `losses = losses + _losses * weight` <br> `return losses.mean(), metrics` | The core DPO loss is calculated within the `self.dpo_loss` method. The provided `get_batch_loss_metrics` function then averages this loss across the batch. The implementation also supports multiple loss types and weighting. The final loss is averaged over the batch. |
